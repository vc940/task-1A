{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12598089,"sourceType":"datasetVersion","datasetId":7936559},{"sourceId":252953894,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.40.0  # or latest\n!pip install torch\n!pip install torchvision\n!pip install PyMuPDF transformers torch\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-28T16:55:20.274940Z","iopub.execute_input":"2025-07-28T16:55:20.275596Z","iopub.status.idle":"2025-07-28T16:55:34.538376Z","shell.execute_reply.started":"2025-07-28T16:55:20.275569Z","shell.execute_reply":"2025-07-28T16:55:34.537362Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.40.0 in /usr/local/lib/python3.11/dist-packages (4.40.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2.32.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.40.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.40.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.40.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.40.0) (2024.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.40.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import LayoutLMTokenizerFast, LayoutLMModel\nfrom transformers import LayoutLMTokenizer, LayoutLMModel\nimport torch\nimport pandas as pd\ndevice = torch.device(\"cuda\")\ntokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\nmodel = LayoutLMModel.from_pretrained(\"microsoft/layoutlm-base-uncased\")\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T16:56:31.606432Z","iopub.execute_input":"2025-07-28T16:56:31.606795Z","iopub.status.idle":"2025-07-28T16:56:32.414776Z","shell.execute_reply.started":"2025-07-28T16:56:31.606761Z","shell.execute_reply":"2025-07-28T16:56:32.413874Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"data =pd.read_csv('/kaggle/input/adobe-dataset-hackathon/dataset.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T16:56:42.782662Z","iopub.execute_input":"2025-07-28T16:56:42.783233Z","iopub.status.idle":"2025-07-28T16:56:48.567477Z","shell.execute_reply.started":"2025-07-28T16:56:42.783207Z","shell.execute_reply":"2025-07-28T16:56:48.566682Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/2224491078.py:1: DtypeWarning: Columns (1,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n  data =pd.read_csv('/kaggle/input/adobe-dataset-hackathon/dataset.csv')\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"data = data.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T16:56:48.569208Z","iopub.execute_input":"2025-07-28T16:56:48.570128Z","iopub.status.idle":"2025-07-28T16:56:49.959748Z","shell.execute_reply.started":"2025-07-28T16:56:48.570099Z","shell.execute_reply":"2025-07-28T16:56:49.959127Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"ids_all_para = data.groupby('id')['level'].apply(lambda x: (x == 'para').all())\n\n# Step 2: Get the list of such ids\nids_to_drop = ids_all_para[ids_all_para].index\n\n# Step 3: Drop all rows with those ids\ndata = data[~data['id'].isin(ids_to_drop)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T16:56:49.960605Z","iopub.execute_input":"2025-07-28T16:56:49.960845Z","iopub.status.idle":"2025-07-28T16:56:50.128725Z","shell.execute_reply.started":"2025-07-28T16:56:49.960826Z","shell.execute_reply":"2025-07-28T16:56:50.127908Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"headings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T10:43:37.369247Z","iopub.execute_input":"2025-07-28T10:43:37.369617Z","iopub.status.idle":"2025-07-28T10:43:37.374808Z","shell.execute_reply.started":"2025-07-28T10:43:37.369592Z","shell.execute_reply":"2025-07-28T10:43:37.374220Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array(['para', 'H1', 'H3', 'H2'], dtype=object)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"headings = data.level.unique()[:4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T16:56:51.774910Z","iopub.execute_input":"2025-07-28T16:56:51.775188Z","iopub.status.idle":"2025-07-28T16:56:51.787856Z","shell.execute_reply.started":"2025-07-28T16:56:51.775166Z","shell.execute_reply":"2025-07-28T16:56:51.787156Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"data = data[data.level.isin(headings)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T16:56:52.347747Z","iopub.execute_input":"2025-07-28T16:56:52.348331Z","iopub.status.idle":"2025-07-28T16:56:52.374552Z","shell.execute_reply.started":"2025-07-28T16:56:52.348305Z","shell.execute_reply":"2025-07-28T16:56:52.373870Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data[].groupby(\"id\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[\"id\"].drop_duplicates().shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:56:17.310250Z","iopub.execute_input":"2025-07-28T05:56:17.310513Z","iopub.status.idle":"2025-07-28T05:56:17.326550Z","shell.execute_reply.started":"2025-07-28T05:56:17.310486Z","shell.execute_reply":"2025-07-28T05:56:17.325748Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(249,)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"data.id.value_counts().hist(bins= 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T08:05:15.042464Z","iopub.execute_input":"2025-07-28T08:05:15.043187Z","iopub.status.idle":"2025-07-28T08:05:15.395417Z","shell.execute_reply.started":"2025-07-28T08:05:15.043161Z","shell.execute_reply":"2025-07-28T08:05:15.394706Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# import ast\n# from collections import defaultdict\n# embeddings = defaultdict(list)\n# labels = defaultdict(list)\n# for idx, row in data.iterrows():\n#     words = str(row.text).split()\n#     box = ast.literal_eval(row.bbox)\n\n#     tokens, token_boxes = [], []\n\n#     for word in words:\n#         word_tokens = tokenizer.tokenize(word)\n#         if len(tokens) + len(word_tokens) >= 510:  # Reserve space for special tokens\n#             break\n#         tokens.extend(word_tokens)\n#         token_boxes.extend([box] * len(word_tokens))\n\n#     # Add special tokens\n#     tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n#     token_boxes = [[0, 0, 0, 0]] + token_boxes + [[0, 0, 0, 0]]\n\n#     input_ids = tokenizer.convert_tokens_to_ids(tokens)\n#     attention_mask = [1] * len(input_ids)\n#     token_type_ids = [0] * len(input_ids)\n#     max_len = 512\n#     padding_length = max_len - len(input_ids)\n\n#     # Padding\n#     input_ids += [tokenizer.pad_token_id] * padding_length\n#     attention_mask += [0] * padding_length\n#     token_type_ids += [0] * padding_length\n#     token_boxes += [[0, 0, 0, 0]] * padding_length\n\n#     # Convert to tensors\n#     input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n#     attention_mask = torch.tensor([attention_mask], dtype=torch.long).to(device)\n#     token_type_ids = torch.tensor([token_type_ids], dtype=torch.long).to(device)\n#     bbox = torch.tensor([token_boxes], dtype=torch.long).to(device)\n\n#     # Get CLS embedding\n#     with torch.no_grad():\n#         outputs = model(\n#             input_ids=input_ids,\n#             bbox=bbox,\n#             attention_mask=attention_mask,\n#             token_type_ids=token_type_ids\n#         )\n#         cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [1, 768]\n        \n#     embeddings[row[\"id\"]].append(cls_embedding.cpu().numpy()[0])\n#     labels[row[\"id\"]].append(row[\"level\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T12:15:22.163218Z","iopub.execute_input":"2025-07-27T12:15:22.163422Z","iopub.status.idle":"2025-07-27T12:15:22.506915Z","shell.execute_reply.started":"2025-07-27T12:15:22.163406Z","shell.execute_reply":"2025-07-27T12:15:22.506094Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import ast\nfrom collections import defaultdict\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\n\nclass BBoxDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        words = str(row.text).split()\n        box = ast.literal_eval(row.bbox)\n        tokens, token_boxes = [], []\n\n        for word in words:\n            word_tokens = self.tokenizer.tokenize(word)\n            if len(tokens) + len(word_tokens) >= self.max_len - 2:  # Reserve space for CLS and SEP\n                break\n            tokens.extend(word_tokens)\n            token_boxes.extend([box] * len(word_tokens))\n\n        # Add special tokens\n        tokens = [self.tokenizer.cls_token] + tokens + [self.tokenizer.sep_token]\n        token_boxes = [[0, 0, 0, 0]] + token_boxes + [[0, 0, 0, 0]]\n\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        attention_mask = [1] * len(input_ids)\n        token_type_ids = [0] * len(input_ids)\n\n        padding_length = self.max_len - len(input_ids)\n\n        # Padding\n        input_ids += [self.tokenizer.pad_token_id] * padding_length\n        attention_mask += [0] * padding_length\n        token_type_ids += [0] * padding_length\n        token_boxes += [[0, 0, 0, 0]] * padding_length\n\n        return {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"bbox\": torch.tensor(token_boxes, dtype=torch.long),\n            \"id\": row[\"id\"],\n            \"level\": row[\"level\"]\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:39:37.381086Z","iopub.execute_input":"2025-07-28T12:39:37.381375Z","iopub.status.idle":"2025-07-28T12:39:37.390009Z","shell.execute_reply.started":"2025-07-28T12:39:37.381354Z","shell.execute_reply":"2025-07-28T12:39:37.389222Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"data[\"length\"] = data[\"text\"].apply(lambda x:len(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:16:54.147064Z","iopub.execute_input":"2025-07-28T14:16:54.147816Z","iopub.status.idle":"2025-07-28T14:16:54.221483Z","shell.execute_reply.started":"2025-07-28T14:16:54.147792Z","shell.execute_reply":"2025-07-28T14:16:54.220936Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = BBoxDataset(data, tokenizer)\ndataloader = DataLoader(dataset, batch_size=256, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:39:38.313802Z","iopub.execute_input":"2025-07-28T12:39:38.314118Z","iopub.status.idle":"2025-07-28T12:39:38.321055Z","shell.execute_reply.started":"2025-07-28T12:39:38.314093Z","shell.execute_reply":"2025-07-28T12:39:38.320156Z"}},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data.id.value_counts().hist(bins = 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T12:15:22.546861Z","iopub.execute_input":"2025-07-27T12:15:22.547078Z","iopub.status.idle":"2025-07-27T12:15:22.748277Z","shell.execute_reply.started":"2025-07-27T12:15:22.547063Z","shell.execute_reply":"2025-07-27T12:15:22.747450Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings = defaultdict(list)\nlabels = defaultdict(list)\n\nmodel.eval()\nwith torch.no_grad():\n    for batch in dataloader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        token_type_ids = batch[\"token_type_ids\"].to(device)\n        bbox = batch[\"bbox\"].to(device)\n        bbox = torch.clamp(bbox, 0, 1000)\n\n        outputs = model(\n            input_ids=input_ids,\n            bbox=bbox,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # shape: [batch_size, hidden_size]\n\n        for i, cls_emb in enumerate(cls_embeddings):\n            id_ = batch[\"id\"][i]\n            level = batch[\"level\"][i]\n            embeddings[id_].append(cls_emb.cpu().numpy())\n            labels[id_].append(level)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:39:44.544524Z","iopub.execute_input":"2025-07-28T12:39:44.544788Z","iopub.status.idle":"2025-07-28T13:43:52.196394Z","shell.execute_reply.started":"2025-07-28T12:39:44.544767Z","shell.execute_reply":"2025-07-28T13:43:52.195820Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"len(embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:04:40.752816Z","iopub.execute_input":"2025-07-28T14:04:40.753488Z","iopub.status.idle":"2025-07-28T14:04:40.758554Z","shell.execute_reply.started":"2025-07-28T14:04:40.753464Z","shell.execute_reply":"2025-07-28T14:04:40.757676Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"705"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"len(data.id.unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T11:35:14.272167Z","iopub.execute_input":"2025-07-28T11:35:14.272717Z","iopub.status.idle":"2025-07-28T11:35:14.291067Z","shell.execute_reply.started":"2025-07-28T11:35:14.272693Z","shell.execute_reply":"2025-07-28T11:35:14.290356Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"705"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit_transform(data.level.unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:04:45.760918Z","iopub.execute_input":"2025-07-28T14:04:45.761202Z","iopub.status.idle":"2025-07-28T14:04:45.774072Z","shell.execute_reply.started":"2025-07-28T14:04:45.761180Z","shell.execute_reply":"2025-07-28T14:04:45.773438Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"array([3, 0, 2, 1])"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"le.classes_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T11:36:35.596916Z","iopub.execute_input":"2025-07-28T11:36:35.597636Z","iopub.status.idle":"2025-07-28T11:36:35.601876Z","shell.execute_reply.started":"2025-07-28T11:36:35.597614Z","shell.execute_reply":"2025-07-28T11:36:35.601232Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"array(['H1', 'H2', 'H3', 'para'], dtype=object)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nX = list(embeddings.values())\ny = list(labels.values())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:04:47.607973Z","iopub.execute_input":"2025-07-28T14:04:47.608215Z","iopub.status.idle":"2025-07-28T14:04:47.782819Z","shell.execute_reply.started":"2025-07-28T14:04:47.608198Z","shell.execute_reply":"2025-07-28T14:04:47.782252Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Example sequence of length 30 tokens\ndef paddend(sequence):\n    # Pad so total length is 300\n    array = np.array(sequence)\n    if len(sequence)>1000:\n        return sequence[:1000]\n    padding_length = 1000 - array.shape[0]\n    \n    # Pad rows at the bottom (right-padding), zero-fill\n    padded_array = np.pad(array, pad_width=((0, padding_length), (0, 0)), mode='constant', constant_values=0)\n    return padded_array\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:04:52.551835Z","iopub.execute_input":"2025-07-28T14:04:52.552078Z","iopub.status.idle":"2025-07-28T14:04:52.570886Z","shell.execute_reply.started":"2025-07-28T14:04:52.552061Z","shell.execute_reply":"2025-07-28T14:04:52.570097Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(X)):\n    X[i] = paddend(X[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:04:55.288708Z","iopub.execute_input":"2025-07-28T14:04:55.289406Z","iopub.status.idle":"2025-07-28T14:04:57.019167Z","shell.execute_reply.started":"2025-07-28T14:04:55.289382Z","shell.execute_reply":"2025-07-28T14:04:57.018362Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"le.classes_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T11:36:39.691497Z","iopub.execute_input":"2025-07-28T11:36:39.692199Z","iopub.status.idle":"2025-07-28T11:36:39.696515Z","shell.execute_reply.started":"2025-07-28T11:36:39.692172Z","shell.execute_reply":"2025-07-28T11:36:39.695892Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"array(['H1', 'H2', 'H3', 'para'], dtype=object)"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"\ndef pad_label(label, max_len=1000):\n    # Ensure label is a list, not a numpy array\n    label = list(label)\n    label = le.transform(label)\n    shape = len(label)\n    if shape > max_len:\n        label = label[:max_len]\n    else:\n        padding = np.array([4]*(max_len -shape))\n        label = np.concatenate([label,padding])\n    return np.array(label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:04:58.764906Z","iopub.execute_input":"2025-07-28T14:04:58.765601Z","iopub.status.idle":"2025-07-28T14:04:58.770400Z","shell.execute_reply.started":"2025-07-28T14:04:58.765574Z","shell.execute_reply":"2025-07-28T14:04:58.769394Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"le.classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T12:31:27.838440Z","iopub.execute_input":"2025-07-27T12:31:27.838721Z","iopub.status.idle":"2025-07-27T12:31:27.884363Z","shell.execute_reply.started":"2025-07-27T12:31:27.838699Z","shell.execute_reply":"2025-07-27T12:31:27.883363Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3713457762.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'LabelEncoder' object has no attribute 'classes'"],"ename":"AttributeError","evalue":"'LabelEncoder' object has no attribute 'classes'","output_type":"error"}],"execution_count":18},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"for i in range(len(y)):\n    y[i] = pad_label(y[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:05:03.184908Z","iopub.execute_input":"2025-07-28T14:05:03.185651Z","iopub.status.idle":"2025-07-28T14:05:03.291880Z","shell.execute_reply.started":"2025-07-28T14:05:03.185628Z","shell.execute_reply":"2025-07-28T14:05:03.291331Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"X = np.array(X)\ny = np.array(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:05:04.602211Z","iopub.execute_input":"2025-07-28T14:05:04.602494Z","iopub.status.idle":"2025-07-28T14:05:06.599767Z","shell.execute_reply.started":"2025-07-28T14:05:04.602461Z","shell.execute_reply":"2025-07-28T14:05:06.599128Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"\nnp.save(\"input.npy\", X)\nnp.save(\"labels.npy\", y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:05:06.601173Z","iopub.execute_input":"2025-07-28T14:05:06.601414Z","iopub.status.idle":"2025-07-28T14:05:08.095198Z","shell.execute_reply.started":"2025-07-28T14:05:06.601388Z","shell.execute_reply":"2025-07-28T14:05:08.094361Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nX = np.load(\"/kaggle/input/adobe-model/input.npy\")\ny = np.load(\"/kaggle/input/adobe-model/labels.npy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:11:27.847564Z","iopub.execute_input":"2025-07-28T17:11:27.848009Z","iopub.status.idle":"2025-07-28T17:11:31.026597Z","shell.execute_reply.started":"2025-07-28T17:11:27.847973Z","shell.execute_reply":"2025-07-28T17:11:31.025718Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"len(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:05:18.628683Z","iopub.execute_input":"2025-07-28T14:05:18.629685Z","iopub.status.idle":"2025-07-28T14:05:18.636448Z","shell.execute_reply.started":"2025-07-28T14:05:18.629642Z","shell.execute_reply":"2025-07-28T14:05:18.635476Z"}},"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"705"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:11:40.633689Z","iopub.execute_input":"2025-07-28T17:11:40.634406Z","iopub.status.idle":"2025-07-28T17:11:40.638990Z","shell.execute_reply.started":"2025-07-28T17:11:40.634375Z","shell.execute_reply":"2025-07-28T17:11:40.638319Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def extract_text_features(text):\n    words = text.split()\n    num_chars = len(text)\n    num_words = len(words)\n    avg_word_len = sum(len(w) for w in words) / max(1, num_words)\n\n    capital_ratio = sum(1 for c in text if c.isupper()) / max(1, num_chars)\n    is_all_caps = int(text.isupper())\n\n    has_colon_or_dot = int(':' in text or '.' in text)\n    ends_with_punct = int(text.strip()[-1] in \".!?\") if len(text.strip()) > 0 else 0\n    starts_with_number = int(text.strip()[0].isdigit()) if len(text.strip()) > 0 else 0\n\n    stopword_count = sum(1 for w in words if w.lower() in stop_words)\n    stopword_ratio = stopword_count / max(1, num_words)\n\n    return pd.Series({\n        'num_chars': num_chars,\n        'num_words': num_words,\n        'avg_word_len': avg_word_len,\n        'capital_ratio': capital_ratio,\n        'is_all_caps': is_all_caps,\n        'has_colon_or_dot': has_colon_or_dot,\n        'ends_with_punct': ends_with_punct,\n        'starts_with_number': starts_with_number,\n        'stopword_ratio': stopword_ratio\n    })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:11:40.923965Z","iopub.execute_input":"2025-07-28T17:11:40.924244Z","iopub.status.idle":"2025-07-28T17:11:40.931521Z","shell.execute_reply.started":"2025-07-28T17:11:40.924225Z","shell.execute_reply":"2025-07-28T17:11:40.930665Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"df_features = data['text'].apply(extract_text_features)\ndata = pd.concat([data, df_features], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:01:15.793010Z","iopub.execute_input":"2025-07-28T17:01:15.793359Z","iopub.status.idle":"2025-07-28T17:01:51.293189Z","shell.execute_reply.started":"2025-07-28T17:01:15.793329Z","shell.execute_reply":"2025-07-28T17:01:51.292534Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"wordfeatures = data[[ 'num_chars','num_words','avg_word_len','capital_ratio','is_all_caps','has_colon_or_dot','ends_with_punct','starts_with_number','stopword_ratio',\"id\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:11:45.532081Z","iopub.execute_input":"2025-07-28T17:11:45.532406Z","iopub.status.idle":"2025-07-28T17:11:45.544854Z","shell.execute_reply.started":"2025-07-28T17:11:45.532382Z","shell.execute_reply":"2025-07-28T17:11:45.544102Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"feature_cols = [col for col in wordfeatures.columns if col != 'id']\n\n# Step 3: Group by 'id' and collect features into sequences (list of rows per group)\ngrouped = wordfeatures.groupby('id')[feature_cols].apply(lambda df: df.to_numpy()).reset_index(name='features')\n\n# Step 4: Pad each group to length 1000 with -1\nMAX_LEN = 1000\nPAD_VALUE = -1\n\ndef pad_sequence(seq, max_len=MAX_LEN, pad_value=PAD_VALUE):\n    length, dim = seq.shape\n    if length >= max_len:\n        return seq[:max_len]\n    else:\n        pad = np.full((max_len - length, dim), pad_value)\n        return np.vstack([seq, pad])\n\n# Step 5: Apply padding\ngrouped['padded_features'] = grouped['features'].apply(pad_sequence)\n\n# Step 6: Convert to final NumPy array (num_docs, 1000, num_features)\nfinal_array = np.stack(grouped['padded_features'].to_numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:11:47.375401Z","iopub.execute_input":"2025-07-28T17:11:47.375682Z","iopub.status.idle":"2025-07-28T17:11:47.663165Z","shell.execute_reply.started":"2025-07-28T17:11:47.375661Z","shell.execute_reply":"2025-07-28T17:11:47.662329Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"final_array.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:10:08.460137Z","iopub.execute_input":"2025-07-28T17:10:08.460932Z","iopub.status.idle":"2025-07-28T17:10:08.465848Z","shell.execute_reply.started":"2025-07-28T17:10:08.460903Z","shell.execute_reply":"2025-07-28T17:10:08.465239Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(705, 1000, 36)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"feats_id = wordfeatures.groupby('id').agg(list).to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:11:53.855264Z","iopub.execute_input":"2025-07-28T17:11:53.855573Z","iopub.status.idle":"2025-07-28T17:11:54.531116Z","shell.execute_reply.started":"2025-07-28T17:11:53.855554Z","shell.execute_reply":"2025-07-28T17:11:54.530543Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:52:21.009569Z","iopub.execute_input":"2025-07-28T14:52:21.009896Z","iopub.status.idle":"2025-07-28T14:52:21.015195Z","shell.execute_reply.started":"2025-07-28T14:52:21.009876Z","shell.execute_reply":"2025-07-28T14:52:21.014354Z"}},"outputs":[{"execution_count":160,"output_type":"execute_result","data":{"text/plain":"list"},"metadata":{}}],"execution_count":160},{"cell_type":"code","source":"feats_id = np.array([np.array(row) for row in feats_id])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:11:56.796439Z","iopub.execute_input":"2025-07-28T17:11:56.796747Z","iopub.status.idle":"2025-07-28T17:11:56.803185Z","shell.execute_reply.started":"2025-07-28T17:11:56.796725Z","shell.execute_reply":"2025-07-28T17:11:56.802322Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"final_array.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:57:38.263306Z","iopub.execute_input":"2025-07-28T14:57:38.263878Z","iopub.status.idle":"2025-07-28T14:57:38.268458Z","shell.execute_reply.started":"2025-07-28T14:57:38.263852Z","shell.execute_reply":"2025-07-28T14:57:38.267713Z"}},"outputs":[{"execution_count":170,"output_type":"execute_result","data":{"text/plain":"(705, 1000, 9)"},"metadata":{}}],"execution_count":170},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:57:25.345807Z","iopub.execute_input":"2025-07-28T14:57:25.346295Z","iopub.status.idle":"2025-07-28T14:57:25.351024Z","shell.execute_reply.started":"2025-07-28T14:57:25.346272Z","shell.execute_reply":"2025-07-28T14:57:25.350332Z"}},"outputs":[{"execution_count":169,"output_type":"execute_result","data":{"text/plain":"(705, 1000, 768)"},"metadata":{}}],"execution_count":169},{"cell_type":"code","source":"X_data = np.concatenate([X,final_array],axis =2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:12:02.196575Z","iopub.execute_input":"2025-07-28T17:12:02.196845Z","iopub.status.idle":"2025-07-28T17:12:05.979946Z","shell.execute_reply.started":"2025-07-28T17:12:02.196828Z","shell.execute_reply":"2025-07-28T17:12:05.979325Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"X_data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:12:05.981026Z","iopub.execute_input":"2025-07-28T17:12:05.981323Z","iopub.status.idle":"2025-07-28T17:12:05.986099Z","shell.execute_reply.started":"2025-07-28T17:12:05.981264Z","shell.execute_reply":"2025-07-28T17:12:05.985574Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"(705, 1000, 804)"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"np.array(a).shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:48:16.883723Z","iopub.execute_input":"2025-07-28T14:48:16.883983Z","iopub.status.idle":"2025-07-28T14:48:16.889051Z","shell.execute_reply.started":"2025-07-28T14:48:16.883962Z","shell.execute_reply":"2025-07-28T14:48:16.888288Z"}},"outputs":[{"execution_count":142,"output_type":"execute_result","data":{"text/plain":"(9, 186)"},"metadata":{}}],"execution_count":142},{"cell_type":"code","source":"feats_id.columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:46:07.249294Z","iopub.execute_input":"2025-07-28T14:46:07.249662Z","iopub.status.idle":"2025-07-28T14:46:07.255194Z","shell.execute_reply.started":"2025-07-28T14:46:07.249631Z","shell.execute_reply":"2025-07-28T14:46:07.254377Z"}},"outputs":[{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"Index(['num_chars', 'num_words', 'avg_word_len', 'capital_ratio',\n       'is_all_caps', 'has_colon_or_dot', 'ends_with_punct',\n       'starts_with_number', 'stopword_ratio'],\n      dtype='object')"},"metadata":{}}],"execution_count":130},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass EmbeddingDataset(Dataset):\n    def __init__(self, X, y):\n        output_features = X[:,:,-9:]\n        X = X[:,:,:-9]\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.output = torch.tensor(output_features,dtype = torch.float32)\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx],self.output[idx]\n\n# Split into train/test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.2, random_state=42)\n\ntrain_ds = EmbeddingDataset(X_train, y_train)\ntest_ds = EmbeddingDataset(X_test, y_test)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_ds, batch_size=32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:10:31.701137Z","iopub.execute_input":"2025-07-28T17:10:31.701762Z","iopub.status.idle":"2025-07-28T17:10:36.626568Z","shell.execute_reply.started":"2025-07-28T17:10:31.701737Z","shell.execute_reply":"2025-07-28T17:10:36.625773Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass TransformerTokenClassifier(nn.Module):\n    def __init__(self, input_dim=768, seq_len=1000, num_classes=5, num_heads=2, num_layers=3, dropout=0.1):\n        super().__init__()\n        # Learnable positional encoding: [1, seq_len, input_dim]\n        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, input_dim))\n        # Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        # Per-token classification head\n        self.classifier = nn.Linear(input_dim, num_classes)\n\n    def forward(self, x, attention_mask=None):\n        # x: [batch_size, seq_len, input_dim]\n        x = x + self.positional_encoding\n\n        # Attention mask for padding (optional, helps mask out padded positions)\n        if attention_mask is not None:\n            # attention_mask: [batch_size, seq_len] -- 1 for real, 0 for pad\n            # Transformer expects mask where True = positions to mask\n            key_padding_mask = ~attention_mask.bool()  # invert\n        else:\n            key_padding_mask = None\n\n        # Pass through transformer encoder\n        x = self.transformer_encoder(x, src_key_padding_mask=key_padding_mask)\n        # Output logits for each token/block\n        logits = self.classifier(x)  # [batch_size, seq_len, num_classes]\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:10:36.627846Z","iopub.execute_input":"2025-07-28T17:10:36.628588Z","iopub.status.idle":"2025-07-28T17:10:36.634605Z","shell.execute_reply.started":"2025-07-28T17:10:36.628565Z","shell.execute_reply":"2025-07-28T17:10:36.633963Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"device\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:11:58.360459Z","iopub.execute_input":"2025-07-27T17:11:58.361196Z","iopub.status.idle":"2025-07-27T17:11:58.370668Z","shell.execute_reply.started":"2025-07-27T17:11:58.361160Z","shell.execute_reply":"2025-07-27T17:11:58.366806Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"le.classes_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T12:31:11.577852Z","iopub.status.idle":"2025-07-27T12:31:11.578106Z","shell.execute_reply.started":"2025-07-27T12:31:11.577982Z","shell.execute_reply":"2025-07-27T12:31:11.577993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y =  data[\"level\"].to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T12:31:11.578489Z","iopub.status.idle":"2025-07-27T12:31:11.578755Z","shell.execute_reply.started":"2025-07-27T12:31:11.578622Z","shell.execute_reply":"2025-07-27T12:31:11.578633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device  = torch.device(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T12:31:39.191073Z","iopub.execute_input":"2025-07-27T12:31:39.191738Z","iopub.status.idle":"2025-07-27T12:31:39.195019Z","shell.execute_reply.started":"2025-07-27T12:31:39.191714Z","shell.execute_reply":"2025-07-27T12:31:39.194220Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T12:31:39.908202Z","iopub.execute_input":"2025-07-27T12:31:39.908474Z","iopub.status.idle":"2025-07-27T12:31:40.118591Z","shell.execute_reply.started":"2025-07-27T12:31:39.908452Z","shell.execute_reply":"2025-07-27T12:31:40.117744Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/65788562.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Example data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_train_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Plot histogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'all_train_labels' is not defined"],"ename":"NameError","evalue":"name 'all_train_labels' is not defined","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"class DeeperMLP(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=512, output_dim=5):\n        super(DeeperMLP, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.15),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.15),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:10:13.444585Z","iopub.execute_input":"2025-07-28T14:10:13.445339Z","iopub.status.idle":"2025-07-28T14:10:13.451401Z","shell.execute_reply.started":"2025-07-28T14:10:13.445307Z","shell.execute_reply":"2025-07-28T14:10:13.450510Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:42:14.536716Z","iopub.execute_input":"2025-07-27T17:42:14.536957Z","iopub.status.idle":"2025-07-27T17:42:14.543769Z","shell.execute_reply.started":"2025-07-27T17:42:14.536942Z","shell.execute_reply":"2025-07-27T17:42:14.542907Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"tensor([65.1604, 58.8061, 51.6615, 89.8826,  0.7554,  0.2168], device='cuda:0')"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:20:18.435275Z","iopub.execute_input":"2025-07-28T14:20:18.435596Z","iopub.status.idle":"2025-07-28T14:20:18.441521Z","shell.execute_reply.started":"2025-07-28T14:20:18.435574Z","shell.execute_reply":"2025-07-28T14:20:18.440753Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CNNPerTimestepMLP(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=768, output_dim=5):\n        super(CNNPerTimestepMLP, self).__init__()\n        clasifier = HeadingClassifier()\n        self.conv1d = nn.Sequential(\n            nn.Conv1d(in_channels=input_dim, out_channels=768, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv1d(768, hidden_dim, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n\n        self.output_layer = \n\n    def forward(self, x):  # x: (batch_size, seq_len=1000, input_dim=768)\n        x = x.permute(0, 2, 1)  # → (batch_size, input_dim=768, seq_len=1000)\n        x = self.conv1d(x)       # → (batch_size, hidden_dim, seq_len=1000)\n        x = x.permute(0, 2, 1)   # → (batch_size, 1000, hidden_dim)\n        output = self.output_layer(x)  # → (batch_size, 1000, output_dim=5)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:14:27.092933Z","iopub.execute_input":"2025-07-28T14:14:27.093210Z","iopub.status.idle":"2025-07-28T14:14:27.099332Z","shell.execute_reply.started":"2025-07-28T14:14:27.093186Z","shell.execute_reply":"2025-07-28T14:14:27.098544Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CNNEmbeddingBranch(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=768):\n        super(CNNEmbeddingBranch, self).__init__()\n        self.conv1d = nn.Sequential(\n            nn.Conv1d(in_channels=input_dim, out_channels=768, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv1d(768, hidden_dim, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n\n    def forward(self, x):  # x: (batch_size, seq_len, input_dim)\n        x = x.permute(0, 2, 1)         # → (batch_size, input_dim, seq_len)\n        x = self.conv1d(x)             # → (batch_size, hidden_dim, seq_len)\n        x = x.permute(0, 2, 1)         # → (batch_size, seq_len, hidden_dim)\n        return x                       # output shape: (batch_size, 1000, hidden_dim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:10:51.566749Z","iopub.execute_input":"2025-07-28T17:10:51.567313Z","iopub.status.idle":"2025-07-28T17:10:51.573301Z","shell.execute_reply.started":"2025-07-28T17:10:51.567275Z","shell.execute_reply":"2025-07-28T17:10:51.572512Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HeadingClassifierWithSizeBranch(nn.Module):\n    def __init__(self, input_dim=768, hidden_dim=768, feat_dim=9, output_dim=5):\n        super(HeadingClassifierWithSizeBranch, self).__init__()\n\n        self.embedding_branch = CNNEmbeddingBranch(input_dim=input_dim, hidden_dim=hidden_dim)\n\n        # Size feature branch — 1 MLP per timestep\n        self.feature_branch = nn.Sequential(\n            nn.Linear(feat_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n\n        # Combine both and classify per timestep\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_dim + 64, hidden_dim),\n        \n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, output_dim)\n        )\n\n    def forward(self, embeddings, size_features):\n        \"\"\"\n        embeddings: (batch, seq_len, 768)\n        size_features: (batch, seq_len, 4)\n        \"\"\"\n        emb_out = self.embedding_branch(embeddings)     # (batch, seq_len, hidden_dim)\n        feat_out = self.feature_branch(size_features)   # (batch, seq_len, 64)\n        combined = torch.cat([emb_out, feat_out], dim=-1)# (batch, seq_len, hidden_dim + 64)\n        out = self.classifier(combined)                 # (batch, seq_len, output_dim)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:10:43.183580Z","iopub.execute_input":"2025-07-28T17:10:43.184079Z","iopub.status.idle":"2025-07-28T17:10:43.190565Z","shell.execute_reply.started":"2025-07-28T17:10:43.184056Z","shell.execute_reply":"2025-07-28T17:10:43.189676Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\nimport torch.nn.functional as F\n\n# Assuming you have a list of all training labels\nall_train_labels = []\nfor _, batch_y,_ in train_loader:\n    all_train_labels.extend(batch_y.view(-1).tolist())\n\n# Compute class weights\nclasses = np.unique(all_train_labels)\nweights = compute_class_weight(class_weight='balanced', classes=classes, y=all_train_labels)\nweights = torch.tensor(weights, dtype=torch.float).to(device)\n\nclass FocalLoss(nn.Module):\n    def __init__(self, weight=None, gamma=3):\n        super().__init__()\n        self.gamma = gamma\n        self.weight = weight\n\n    def forward(self, inputs, targets):\n        logp = F.log_softmax(inputs, dim=1)\n        ce_loss = F.nll_loss(logp, targets, reduction='none', weight=self.weight)\n        p = torch.exp(-ce_loss)\n        loss = ((1 - p) ** self.gamma * ce_loss).mean()\n        return loss\n\ncriterion = FocalLoss(weight=weights)\n\n# Add weights to loss\n\nfrom sklearn.metrics import accuracy_score\nfrom torch.optim import Adam\nfrom tqdm import tqdm\n\nmodel = HeadingClassifierWithSizeBranch().to(device)\noptimizer = Adam(model.parameters(), lr=1e-3)\nscheduler = ReduceLROnPlateau(\n    optimizer,\n    mode='min',       # or 'max' if monitoring accuracy\n    factor=0.5,       # reduce LR by this factor\n    patience=4,       # epochs to wait before reducing LR\n    min_lr=1e-8,      # minimum LR\n    verbose=True      # print update messages\n)\n\nnum_epochs = 100\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", leave=False)\n\n    for batch_x, batch_y,feats in train_bar:\n        batch_x = batch_x.to(device)\n        batch_y = batch_y.to(device)\n        feats = feats.to(device)\n        outputs = model(batch_x,feats)  # shape: (batch_size, 768, ...)\n        outputs = outputs.view(-1, 5)\n        batch_y = batch_y.view(-1)\n\n        loss = criterion(outputs, batch_y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        train_bar.set_postfix(loss=loss.item())\n\n    # Evaluation\n    model.eval()\n    all_preds, all_labels = [], []\n    test_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Testing]\", leave=False)\n    testloss = 0\n    with torch.no_grad():\n        for test_x, test_y,feats in test_bar:\n            test_x, test_y,feats = test_x.to(device), test_y.to(device),feats.to(device)\n\n            test_outputs = model(test_x,feats)\n            test_outputs = test_outputs.view(-1, 5)\n            test_y = test_y.view(-1)\n\n            test_preds = torch.argmax(test_outputs, dim=1)\n            all_preds.extend(test_preds.cpu().numpy())\n            all_labels.extend(test_y.cpu().numpy())\n            testloss += criterion(test_outputs, test_y)\n    \n    acc = accuracy_score(all_labels, all_preds)\n    scheduler.step(testloss)\n    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {running_loss/len(train_loader):.4f} | Test Acc: {acc:.4f} testloss: {testloss/len(test_loader)}\")\n    cm = confusion_matrix(all_labels, all_preds)\n    print(cm)\n    print(classification_report(all_labels,all_preds))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:10:55.663227Z","iopub.execute_input":"2025-07-28T17:10:55.663967Z","iopub.status.idle":"2025-07-28T17:10:59.563444Z","shell.execute_reply.started":"2025-07-28T17:10:55.663943Z","shell.execute_reply":"2025-07-28T17:10:59.562257Z"}},"outputs":[{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/168466359.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (batch_size, 768, ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/3949472054.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, size_features)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msize_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0memb_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# (batch, seq_len, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mfeat_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_features\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (batch, seq_len, 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# (batch, seq_len, hidden_dim + 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/2750414371.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# x: (batch_size, seq_len, input_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# → (batch_size, input_dim, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# → (batch_size, hidden_dim, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# → (batch_size, seq_len, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m                       \u001b[0;31m# output shape: (batch_size, 1000, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n\u001b[0;32m--> 370\u001b[0;31m         return F.conv1d(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         )\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [768, 768, 3], expected input[32, 795, 1000] to have 768 channels, but got 795 channels instead"],"ename":"RuntimeError","evalue":"Given groups=1, weight of size [768, 768, 3], expected input[32, 795, 1000] to have 768 channels, but got 795 channels instead","output_type":"error"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(all_labels, all_preds)\nprint(cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:28:43.015806Z","iopub.execute_input":"2025-07-27T17:28:43.016407Z","iopub.status.idle":"2025-07-27T17:28:43.042604Z","shell.execute_reply.started":"2025-07-27T17:28:43.016388Z","shell.execute_reply":"2025-07-27T17:28:43.041928Z"}},"outputs":[{"name":"stdout","text":"[[   77    14    18    14     0     0]\n [  137    66    17    34     0     0]\n [   19    16    14    43     0     0]\n [    7    90     4     6     0     0]\n [ 4835  2828  1326  5006     0     0]\n [ 1497  3270  8041 22377     0   244]]\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report\n\ncm = confusion_matrix(all_labels, all_preds)\nprint(cm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T15:05:42.811076Z","iopub.execute_input":"2025-07-28T15:05:42.811358Z","iopub.status.idle":"2025-07-28T15:05:42.872382Z","shell.execute_reply.started":"2025-07-28T15:05:42.811336Z","shell.execute_reply":"2025-07-28T15:05:42.871267Z"}},"outputs":[{"name":"stdout","text":"[[   174     97     31     69      0]\n [    65    213    140    175      0]\n [    37    174    189    107      0]\n [  1034   2255   2372  14018      1]\n [     1      0      3      0 119845]]\n","output_type":"stream"}],"execution_count":185},{"cell_type":"code","source":"le.classes_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T11:43:39.030613Z","iopub.execute_input":"2025-07-28T11:43:39.031108Z","iopub.status.idle":"2025-07-28T11:43:39.035693Z","shell.execute_reply.started":"2025-07-28T11:43:39.031085Z","shell.execute_reply":"2025-07-28T11:43:39.034905Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"array(['H1', 'H2', 'H3', 'para'], dtype=object)"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remap_labels(label):\n    if label in [0, 1, 2]:\n        return 0\n    elif label == 3:\n        return 1\n    elif label == 4:\n        return 1\n    else:\n        return label  # fallback for unexpected values\n\n# Apply to list\nlabels = all_labels\nall_labels = [remap_labels(l) for l in labels]\nlabels = all_preds\nall_preds = [remap_labels(l) for l in labels]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T15:23:14.210367Z","iopub.execute_input":"2025-07-28T15:23:14.211076Z","iopub.status.idle":"2025-07-28T15:23:14.292379Z","shell.execute_reply.started":"2025-07-28T15:23:14.211041Z","shell.execute_reply":"2025-07-28T15:23:14.291842Z"}},"outputs":[],"execution_count":194},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(classification_report(all_labels,all_preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T15:23:18.871945Z","iopub.execute_input":"2025-07-28T15:23:18.872540Z","iopub.status.idle":"2025-07-28T15:23:18.965612Z","shell.execute_reply.started":"2025-07-28T15:23:18.872512Z","shell.execute_reply":"2025-07-28T15:23:18.964849Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.15      0.76      0.25      1471\n           1       1.00      0.95      0.98    139529\n\n    accuracy                           0.95    141000\n   macro avg       0.57      0.86      0.61    141000\nweighted avg       0.99      0.95      0.97    141000\n\n","output_type":"stream"}],"execution_count":195},{"cell_type":"code","source":"data.level.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T18:17:06.079304Z","iopub.execute_input":"2025-07-27T18:17:06.079562Z","iopub.status.idle":"2025-07-27T18:17:06.091340Z","shell.execute_reply.started":"2025-07-27T18:17:06.079544Z","shell.execute_reply":"2025-07-27T18:17:06.090818Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"level\npara    84020\nH2        855\nH3        827\nH1        714\nH4        682\nName: count, dtype: int64"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}